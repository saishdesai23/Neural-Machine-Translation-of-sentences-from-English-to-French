{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPB464/x7gDsGbcJq0Q3GMV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bAbSGIEn8uOE"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","from tqdm.notebook import tqdm\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n","from positionalembedding import create_positional_embedding\n","\n","def decode_transformer_model(encoder, decoder, src, max_decode_len, device):\n","    \"\"\"\n","    Args:\n","        encoder: Your TransformerEncoder object\n","        decoder: Your TransformerDecoder object\n","        src: [max_src_length, batch_size] the source sentences you wish to translate\n","        max_decode_len: The maximum desired length (int) of your target translated sentences\n","        device: the device your torch tensors are on (you may need to call x.to(device) for some of your tensors)\n","\n","    Returns:\n","        curr_output: [batch_size, max_decode_len] containing your predicted translated sentences\n","        curr_predictions: [batch_size, max_decode_len, trg_vocab_size] containing the (unnormalized) probabilities of each\n","            token in your vocabulary at each time step\n","    \"\"\"\n","    # Initializing variables\n","    trg_vocab = decoder.trg_vocab\n","    batch_size = src.size(1)\n","    curr_output = torch.zeros((batch_size, max_decode_len))\n","    curr_predictions = torch.zeros((batch_size, max_decode_len, len(trg_vocab.idx2word)))\n","    enc_output = None\n","\n","    # Decoding the start token for each example\n","    dec_input = torch.tensor([[trg_vocab.word2idx['<start>']]] * batch_size).transpose(0,1)\n","    curr_output[:, 0] = dec_input.squeeze(1)\n","    enc_output = encoder.forward(src)\n","    for t in range(1, max_decode_len):\n","      next_token = curr_output[:,:t].to(torch.long)\n","      decoder_output = decoder.forward(next_token.transpose(0,1), enc_output)\n","      curr_predictions[:,t,:] = decoder_output.permute(1, 0, 2)[:,-1,:]\n","      decoder_output = torch.argmax(curr_predictions[:,t,:], dim = -1)\n","\n","      curr_output[:, t] = decoder_output\n","    return curr_output, curr_predictions, enc_output"]}]}